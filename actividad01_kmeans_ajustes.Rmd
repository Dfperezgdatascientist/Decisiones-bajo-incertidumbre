---
title: "Análisis de la estabilidad de los centroides en K-Medias en presencia de correlación"
author: "*Daniel Felipe Pérez Grajales* <br/> *Universidad Nacional de Colombia - Sede Medellín* <br/><br/> *Efraín Galvis Amaya* <br/> *Universidad Nacional de Colombia - Sede Medellín* <br/> <br/> **Profesor**: *Juan David Ospina Arango* <br/> *Universidad Nacional de Colombia - Sede Medellín* <br/> *Departamento de Ciencias de la Computación y de la Decisión* <br/> *Decisiones bajo incertidumbre (Optimización para aprendizaje de máquina)*"

date: "Semestre 2021-01"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(mvtnorm)
library(MBESS)
library(Matrix)
library(tidyverse)
```

### El algoritmo de K-Means

El algoritmo de [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) es una técnica popular de aprendizaje no supervisado para agrupar observaciones. 

Uno de los retos en la aplicación de los métodos de aprendizaje de máquinas es el manejo de información redundante. Se considera que la información es redundante cuando a partir de unas variables se pueden inferir las otras.

Un ejemplo de redundancia es la correlación alta entre variables. Si dos variables están altamente correlacionadas, conocer lo que pasa con una permite saber lo que pasa con la otra. Este problema también se conoce como colinealidad. 

Por otro lado, la estabilidad de un método de aprendizaje de máquina se puede entender de diferentes maneras. Cambios pequeños en el conjunto de entrenamiento no producen cambios significativos en:
* a) en los parámetros estimados del modelo (estabilidad en los parámetros) o 
* b) en las salidas del modelo (cambio en las predicciones del modelo)

Uno de los retos de la redundancia es que puede afectar la estabilidad de los métodos de aprendizaje de máquina. En particular, en K-Medias la estabilidad se puede establecer como la variabilidad de los centroides finales cada vez que se cambian los centroides iniciales. Cuando cambiar los centroides iniciales no modifica los centroides finales, se puede considerar que el método tiene un comportamiento estable respecto a la inicialización.


### Objetivo
Entender cómo la correlación entre las variables numéricas puede afectar la estabilidad de los centroides en el algoritmo de K-Medias utilizando escenarios de simulación.

### Retos de aprendizaje
* Planteamiento de estudios de simulación
* Refuerzo de los conceptos estadísticos de media, varianza, covarianza y correlación, distribución normal multivariada
* Refuerzo del algoritmo de K-Medias


### Metodología
Se deberá desarrollar un experimento de simulación para analizar la estabilidad del algoritmo de K-Means. Para ello se proponen los siguientes pasos.

### 1. Simular tres grupos de distribuciones normales bivariadas independientes pero con traslape. Es decir que los miembros de cada grupo son $X\sim N_2(\mu_j,\Sigma_j)$, $j=1,2,3$. A continuación se presenta un ejemplo de dos grupos generados a partir de distribuciones normales bivariadas:

```{r , echo=F}
M_cor<-matrix(c(1,0.6,0.6,1),ncol=2)
M_cov<-cor2cov(M_cor,sd=c(1,1))
M_cov_pd<-as.matrix(nearPD(M_cov)$mat)

n1<-50 # Tamaño de la muestra de la clase 1
n2<-100 # Tamaño de la muestra de la clase 2
n3<- 150  #tamaño de la muestra de la clase 3
mu1<-c(-1.5,1.5) # Vector de medias de la clase 1
mu2<-c(1,1.5) # Vector de medias de la clase 2
mu3<-c(4,1.5) # vector de medias de la clase 3

set.seed(44)
muestra1<-rmvnorm(n=n1,mean=mu1,sigma=M_cov_pd,method="eigen")
muestra2<-rmvnorm(n=n2,mean=mu2,sigma=M_cov_pd,method="eigen")
muestra3<-rmvnorm(n=n3,mean=mu3,sigma=M_cov_pd,method="eigen")
muestra_compl<-rbind(muestra1,muestra2, muestra3)
colnames(muestra_compl) <- c("x1", "x2")

clase<-c(rep(-1,n1),rep(1,n2), rep(2,n3))
muestra_compl_df<-data.frame(muestra_compl,clase)

plot(muestra_compl,
     col=(clase+2),
     pch=(clase+2),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="3 conjuntos de datos con dist Bivariada",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Clase 1", "Clase 2", "Clase 3"),
       pch=c(1,3,4),col=c(1,3,4),pt.lwd=2,pt.cex=1.8,bty="n")
```

### 2. Encontrar los centroides con K-Means fijando el método de inicialización de los centroides. Encuentre los centroides para $n_c$ inicializaciones aleatorias.

```{r , echo=F}
set.seed(1)
km <- kmeans(muestra_compl_df[,-3], 3)

print("A continuación los centroidoes")

km$centers

```

Luegos vemos los centroides en un plano cartesiano

```{r , echo=F}
plot(km$centers[order(km$centers[,1]),],
     col=c(1,3,4),
     pch=c(1,3,4),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="3 conjuntos de datos con dist Bivariada",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Clase 1", "Clase 2", "Clase 3"),
       pch=c(1,3,4),col=c(1,3,4),pt.lwd=2,pt.cex=1.8,bty="n")

```

### 3. Suponga que el $i$-ésimo individuo es de la forma $X_i = [x_1^i \quad x_2^i]^T$. Cree la variable $x_3$ como $x_3^i=x_1^i+\epsilon_i$ con $\epsilon_i$ iid de media cero y varianza constante. ¿Cuál es la varianza de $x_3$? ¿Cuál es la covarianza entre $x_1$ y $x_3$? ¿Al agregar esta variable K-Means sigue detectando correctamente los centroides? ¿Qué pasa con la estabilidad de los centroides cuando la varianza de $\epsilon$ aumenta?

Creamos $x_3$ con $\epsilon_i$ iid de media cero y varianza constante.

```{r , echo=F}
ei <- rnorm(n = dim(muestra_compl)[1], mean = 0, sd = 5)
x3 <- muestra_compl[,1] + ei
muestra_compl_df <- cbind(muestra_compl, x3)

```


* Calculamos varianza de $x_3$
```{r , echo=F}
var(muestra_compl_df[,3])
```

* Calculamos covarianza entre $x_1$ y $x_3$

```{r , echo=F}
cov(muestra_compl_df[,1], muestra_compl_df[,3])
```

* Calculamos la matriz de correlaciones
```{r , echo=F}
cor(muestra_compl_df)
```

* ¿Al agregar esta variable K-Means sigue detectando correctamente los centroides?

```{r , echo=F}
set.seed(44)
km1 <- kmeans(muestra_compl_df, centers = 3)

print("A continuación los centroidoes despues de cambio")

km1$centers
```


Luegos vemos los centroides en un plano cartesiano

```{r , echo=F}
plot(km1$centers[order(km1$centers[,1]),],
     col=c(1,3,4),
     pch=c(1,3,4),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="3 conjuntos de datos con dist Bivariada",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Clase 1", "Clase 2", "Clase 3"),
       pch=c(1,3,4),col=c(1,3,4),pt.lwd=2,pt.cex=1.8,bty="n")

```


Se puede ver claramente un primer cambio en los centroides, cuando se aumenta la varianza se traslapan mas los centroides.


* ¿Qué pasa con la estabilidad de los centroides cuando la varianza de $\epsilon$ aumenta?

Aumentamos la varianza a 1000
```{r , echo=F}
# Aumentando varianza #
ei <- rnorm(n = dim(muestra_compl)[1], mean = 0, sd = 1000)
x3 <- muestra_compl[,1] + ei
muestra_compl_df <- cbind(muestra_compl, x3)

```


```{r , echo=F}
set.seed(44)
km2 <- kmeans(muestra_compl_df, centers = 3)

print("A continuación los centroidoes despues de nuevo cambio")

km2$centers
```


Luegos vemos los centroides en un plano cartesiano

```{r , echo=F}
plot(km2$centers[order(km2$centers[,1]),],
     col=c(1,3,4),
     pch=c(1,3,4),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="3 conjuntos de datos con dist Bivariada",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Clase 1", "Clase 2", "Clase 3"),
       pch=c(1,3,4),col=c(1,3,4),pt.lwd=2,pt.cex=1.8,bty="n")

```


Se concluye que despues de aumenta la varianza de los datos, éstos tienden a agruparse, perjudicando que el modelo logre crear 3 grupos independientes entre ellos.

### 4. Como en el paso anterior, cree las variables $x_4$ y $x_6$ como la suma de $x_2$ y otra variable de media cero y varianza constante y la variable $x_5$ como la suma de $x_3$ y otra variable de media cero y varianza constante. ¿Al agregar estas variables, K-Means sigue detectando correctamente los centroides? ¿Qué pasa cuando la estabilidad de los centroides cuando la varianza de las variables que se suman a las variables originales aumenta?


* Primero crearmos la variables $x_4$ y $x_5$
```{r , echo=F}
ei1 <- rnorm(n = dim(muestra_compl)[1], mean = 0, sd = 6)
ei2 <- rnorm(n = dim(muestra_compl)[1], mean = 0, sd = 7)
x4 <- muestra_compl_df[,2] + ei1
x5 <- muestra_compl_df[,3] + ei2
muestra_compl_df1 <- cbind(muestra_compl_df, x4, x5)

```
 
* Analicemos sus correlaciones
```{r , echo=F}
cor(muestra_compl_df1)
```

se puede evidenciar una alta correlacion entre las variables x3, x4 y x5 ya que estar cercanos a 1, lo que quiere decir que hay una dependencia lineal entre las variables.


* ¿Al agregar esta variable K-Means sigue detectando correctamente los centroides?

```{r , echo=F}
set.seed(44)
km3 <- kmeans(muestra_compl_df1, centers = 3)

print("A continuación los centroidoes despues de nuevo cambio")

km3$centers
```


Luegos veamos los centroides en un plano cartesiano

```{r , echo=F}
plot(km3$centers[order(km3$centers[,1]),],
     col=c(1,3,4),
     pch=c(1,3,4),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="3 conjuntos de datos con dist Bivariada",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Clase 1", "Clase 2", "Clase 3"),
       pch=c(1,3,4),col=c(1,3,4),pt.lwd=2,pt.cex=1.8,bty="n")

```


Se concluye que al existir influiencia de variables altamente correlacionadas tambien afectan la estabilidad de los centroides, es decir, al modelo le cuesta separar correctamente los grupos, por su dependencia lineal, es igual que este una o la otra variable correlacionada por eso se evidencia como que estuvieran un centro enciama del otro.


* ¿Qué pasa cuando la estabilidad de los centroides cuando la varianza de las variables que se suman a las variables originales aumenta?

```{r , echo=F}
ei1 <- rnorm(n = dim(muestra_compl)[1], mean = 0, sd = 2500)
ei2 <- rnorm(n = dim(muestra_compl)[1], mean = 0, sd = 5000)
x4 <- muestra_compl_df[,2] + ei1
x5 <- muestra_compl_df[,3] + ei2
muestra_compl_df2 <- cbind(muestra_compl_df, x4, x5)
km4 <- kmeans(muestra_compl_df2, centers = 3)
print(km4$centers)
plot(km4$centers[order(km4$centers[,1]),],
     col=c(1,3,4),
     pch=c(1,3,4),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="3 conjuntos de datos con dist Bivariada",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Clase 1", "Clase 2", "Clase 3"),
       pch=c(1,3,4),col=c(1,3,4),pt.lwd=2,pt.cex=1.8,bty="n")


```

Se concluye que aumentos en varianza, nuevamente afecta la manera como el modelo busca los centroides, afectando el proceso de optimización.


### 5. A partir de estos experimentos, ¿qué se podría decir del efecto de la correlación entre variables y la estabilidad de los centroides en K-Medias?


* Al agregar nuevas variables durante la simulación,las variable eran altamente correlacionadas porque eran simuladas a partir de una variable ya conocida y se le sumaba una variable iid con media cero y desviación estandar constante, afectaba el cálculo de los centroides de los cluester, ese cambio era mas fuerte a cuando se aumentaba la varianza provocando que se solapen los centroides y afectando la opmiziacion del ejercicio, variables altamente correlacionadas significa una dependencia lineal,por lo que es indiferente la presencia de una u otra, al construir los centroides eran muy parecidos, es recomendable dejar una sola variable de las dos correlacionadas para estabilizar los centroides.









